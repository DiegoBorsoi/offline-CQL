{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Azure - Offline CQL - SAC - ant-bullet.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"QEFS2tL95-OM"},"source":["## Initial Settings"]},{"cell_type":"code","metadata":{"id":"Snu6NXuYrM-x"},"source":["!pip install git+https://github.com/takuseno/d4rl-pybullet > /dev/null 2>&1\n","!pip install pybullet > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o44AS-NGrM-0"},"source":["import random\n","\n","import gym\n","import pybullet\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.distributions import Normal\n","\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","import d4rl_pybullet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oGDaDZ9rIXrF"},"source":["#from google.colab import drive\r\n","#drive.mount(\"./drive\")\r\n","\r\n","import os\r\n","\r\n","BASE_PATH = \"./Saves/\"\r\n","\r\n","try:\r\n","  os.mkdir(BASE_PATH)\r\n","except:\r\n","  pass\r\n","\r\n","TEST_NAME = \"SAC-ant-pybullet\"\r\n","\r\n","try:\r\n","  os.mkdir(BASE_PATH + \"checkpoints/\")\r\n","except:\r\n","  pass\r\n","\r\n","CHECKPOINT_PATH = BASE_PATH + \"checkpoints/\" + TEST_NAME\r\n","TENSORBOARD_PATH = BASE_PATH + \"runs/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNWrWzq7rM-0"},"source":["## Set random seed"]},{"cell_type":"code","metadata":{"id":"lbkSrTh_rM-0"},"source":["if torch.backends.cudnn.enabled:\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","seed = 1\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X1mweQVIrM-3"},"source":["## Network"]},{"cell_type":"code","metadata":{"id":"j4huQKKzrM-3"},"source":["def init_layer_uniform(layer: nn.Linear, init_w: float = 3e-3) -> nn.Linear:\n","    \"\"\"Init uniform parameters on the single layer.\"\"\"\n","    layer.weight.data.uniform_(-init_w, init_w)\n","    layer.bias.data.uniform_(-init_w, init_w)\n","\n","    return layer\n","\n","\n","class Actor(nn.Module):\n","    def __init__(\n","        self, \n","        in_dim: int, \n","        out_dim: int,\n","        log_std_min: float = -20,\n","        log_std_max: float = 2,\n","    ):\n","        \"\"\"Initialize.\"\"\"\n","        super(Actor, self).__init__()\n","        \n","        # set the log std range\n","        self.log_std_min = log_std_min\n","        self.log_std_max = log_std_max\n","        \n","        # set the hidden layers\n","        self.hidden1 = nn.Linear(in_dim, 256)\n","        self.hidden2 = nn.Linear(256, 256)\n","        \n","        # set log_std layer\n","        self.log_std_layer = nn.Linear(256, out_dim)\n","        self.log_std_layer = init_layer_uniform(self.log_std_layer)\n","\n","        # set mean layer\n","        self.mu_layer = nn.Linear(256, out_dim)\n","        self.mu_layer = init_layer_uniform(self.mu_layer)\n","\n","    def forward(self, state: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward method implementation.\"\"\"\n","        x = F.relu(self.hidden1(state))\n","        x = F.relu(self.hidden2(x))\n","        \n","        # get mean\n","        mu = self.mu_layer(x).tanh()\n","        \n","        # get std\n","        log_std = self.log_std_layer(x).tanh()\n","        log_std = self.log_std_min + 0.5 * (\n","            self.log_std_max - self.log_std_min\n","        ) * (log_std + 1)\n","        std = torch.exp(log_std)\n","        \n","        # sample actions\n","        dist = Normal(mu, std)\n","        z = dist.rsample()\n","        \n","        # normalize action and log_prob\n","        action = z.tanh()\n","        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-7)\n","        log_prob = log_prob.sum(-1, keepdim=True)\n","        \n","        return action, log_prob\n","    \n","    \n","class CriticQ(nn.Module):\n","    def __init__(self, in_dim: int):\n","        \"\"\"Initialize.\"\"\"\n","        super(CriticQ, self).__init__()\n","        \n","        self.hidden1 = nn.Linear(in_dim, 256)\n","        self.hidden2 = nn.Linear(256, 256)\n","        self.out = nn.Linear(256, 1)\n","        self.out = init_layer_uniform(self.out)\n","\n","    def forward(\n","        self, state: torch.Tensor, action: torch.Tensor\n","    ) -> torch.Tensor:\n","        \"\"\"Forward method implementation.\"\"\"\n","        x = torch.cat((state, action), dim=-1)\n","        x = F.relu(self.hidden1(x))\n","        x = F.relu(self.hidden2(x))\n","        value = self.out(x)\n","        \n","        return value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EIzqOZ1jrM-4"},"source":["## SAC Agent"]},{"cell_type":"code","metadata":{"id":"lcW-wL80rM-4"},"source":["class SACAgent:\n","    \"\"\"SAC agent interacting with environment.\n","    \n","    Attrtibutes:\n","        actor (nn.Module): actor model to select actions\n","        actor_optimizer (Optimizer): optimizer for training actor\n","        qf_1 (nn.Module): critic model to predict state-action values\n","        qf_2 (nn.Module): critic model to predict state-action values\n","        qf_1_target (nn.Module): target critic model to predict state-action values\n","        qf_2_target (nn.Module): target critic model to predict state-action values\n","        qf_1_optimizer (Optimizer): optimizer for training qf_1\n","        qf_2_optimizer (Optimizer): optimizer for training qf_2\n","        state_dim (int): dimension of the state space,\n","        action_dim (int): dimension of the action space,\n","        dataset (dict of numpy): dataset from d4rl of tuple (s,a,s',r,done)\n","        batch_size (int): batch size for sampling\n","        gamma (float): discount factor\n","        tau (float): parameter for soft target update\n","        policy_update_freq (int): policy update frequency\n","        device (torch.device): cpu / gpu\n","        target_entropy (int): desired entropy used for the inequality constraint\n","        log_alpha (torch.Tensor): weight for entropy\n","        alpha_optimizer (Optimizer): optimizer for alpha\n","        log_alpha_cql (torch.Tensor): weight for Conservative part\n","        alpha_cql_optimizer (Optimizer): optimizer for alpha_cql\n","        cql_threshold (int): threshold for the alpha_cql maximization\n","        total_step (int): total step numbers\n","    \"\"\"\n","    \n","    def __init__(\n","        self,\n","        dataset,\n","        state_dim,\n","        action_dim,\n","        batch_size: int,\n","        gamma: float = 0.99,\n","        tau: float = 5e-3,\n","        policy_update_freq: int = 1,\n","        cql_threshold=10,\n","        random_actions_num=10\n","    ):\n","        \"\"\"Initialize.\"\"\"\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","\n","        self.batch_size = batch_size\n","        self.gamma = gamma\n","        self.tau = tau\n","        self.policy_update_freq = policy_update_freq\n","        self.random_actions_num = random_actions_num\n","\n","        # device: cpu / gpu\n","        self.device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        )\n","        print(self.device)\n","        \n","        # automatic entropy tuning\n","        self.target_entropy = -np.prod((self.action_dim,)).item()  # heuristic\n","        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n","        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=3e-4)\n","\n","        # with Lagrange alpha\n","        self.cql_threshold = cql_threshold\n","        self.log_alpha_cql = torch.zeros(1, requires_grad=True, device=self.device)\n","        self.alpha_cql_optimizer = optim.Adam([self.log_alpha_cql], lr=3e-4)\n","\n","        # actor\n","        self.actor = Actor(self.state_dim, self.action_dim).to(self.device)\n","        \n","        # q function\n","        self.qf_1 = CriticQ(self.state_dim + self.action_dim).to(self.device)\n","        self.qf_2 = CriticQ(self.state_dim + self.action_dim).to(self.device)\n","        self.qf_1_target = CriticQ(self.state_dim + self.action_dim).to(self.device)\n","        self.qf_2_target = CriticQ(self.state_dim + self.action_dim).to(self.device)\n","        self.qf_1_target.load_state_dict(self.qf_1.state_dict())\n","        self.qf_2_target.load_state_dict(self.qf_2.state_dict())\n","\n","        # optimizers\n","        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-5)\n","        self.qf_1_optimizer = optim.Adam(self.qf_1.parameters(), lr=3e-4)\n","        self.qf_2_optimizer = optim.Adam(self.qf_2.parameters(), lr=3e-4)\n","        \n","        # total steps count\n","        self.total_step = 0\n","    \n","    \n","    def select_action(self, state: np.ndarray) -> np.ndarray:\n","        \"\"\"Select an action from the input state.\"\"\"\n","        selected_action = self.actor(torch.FloatTensor(state).to(self.device))[0].detach().cpu().numpy()\n","        \n","        return selected_action\n","\n","    def get_policy_actions(self, states, random_actions_num):\n","        \"\"\"Returns the action chosen by the actor one th input states, repeated random_actions_num times\"\"\"\n","        states_resized = states.unsqueeze(1).repeat(1, random_actions_num, 1).view(states.shape[0] * random_actions_num, states.shape[-1])\n","        actions, log_prob = self.actor(states_resized)\n","        return actions, log_prob.view(states.shape[0], random_actions_num, 1)\n","    \n","    def get_q_from_actions(self, states, actions, random_actions_num):\n","        \"\"\"\n","        Returns the value of the function Q (in particular Q1 and Q2) using the critics networks on the input state and actions.\n","        states: shape = (BATCH_SIZE, space_dim)\n","        actions: shape = (BATCH_SIZE * random_actions_num)\n","        \"\"\"\n","        states_resized = states.unsqueeze(1).repeat(1, random_actions_num, 1).view(states.shape[0] * random_actions_num, states.shape[-1])\n","        q1 = self.qf_1(states_resized, actions)\n","        q2 = self.qf_2(states_resized, actions)\n","        return q1.view(states.shape[0], random_actions_num, 1), q2.view(states.shape[0], random_actions_num, 1)\n","    \n","    def update_model(self):\n","        \"\"\"Update the model by gradient descent.\"\"\"\n","        device = self.device  # for shortening the following lines\n","        \n","        samples = sample_batch(self.batch_size)\n","        states = torch.FloatTensor(samples[\"state\"]).to(device)\n","        next_states = torch.FloatTensor(samples[\"next_state\"]).to(device)\n","        actions = torch.FloatTensor(samples[\"action\"]).to(device)\n","        rewards = torch.FloatTensor(samples[\"reward\"].reshape(-1, 1)).to(device)\n","        dones = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n","\n","        new_actions, log_prob = self.actor(states)\n","        \n","        # train alpha (dual problem)\n","        alpha_loss = (\n","            -self.log_alpha * (log_prob + self.target_entropy).detach()\n","        ).mean()\n","\n","        self.alpha_optimizer.zero_grad()\n","        alpha_loss.backward()\n","        self.alpha_optimizer.step()\n","        \n","        alpha = self.log_alpha.exp()  # used for the actor loss calculation\n","\n","        q_pred = torch.min(\n","            self.qf_1(states, new_actions), self.qf_2(states, new_actions)\n","        )\n","        \n","        # actor loss\n","        actor_loss = (alpha * log_prob - q_pred).mean()\n","        \n","        # q function loss\n","        mask = 1 - dones\n","        q_1_pred = self.qf_1(states, actions)\n","        q_2_pred = self.qf_2(states, actions)\n","\n","        target_actions, target_actions_log_prob = self.actor(next_states)\n","        q_target = torch.min(\n","            self.qf_1_target(next_states, target_actions), self.qf_2_target(next_states, target_actions)\n","        ) - alpha * target_actions_log_prob\n","        q_target = rewards + self.gamma * q_target * mask\n","\n","        # calculate the mean squared error loss\n","        qf_1_loss_mse = F.mse_loss(q_1_pred, q_target.detach())\n","        qf_2_loss_mse = F.mse_loss(q_2_pred, q_target.detach())\n","        \n","\n","        # ---- CQL part -------------------------------------------------------------------------------------------------------------------------------------- #\n","        random_actions = torch.FloatTensor(self.batch_size * self.random_actions_num, actions.shape[-1]).uniform_(-1, 1).to(device)\n","        policy_actions, policy_actions_log_prob = self.get_policy_actions(states, self.random_actions_num)\n","\n","        q1_random, q2_random = self.get_q_from_actions(states, random_actions, self.random_actions_num)  # shape: (BATCH_SIZE, self.random_actions_num, 1)\n","        q1_policy, q2_policy = self.get_q_from_actions(states, policy_actions, self.random_actions_num)\n","\n","        # importance sampling (continuous variant of log_sum_exp)\n","        random_density = np.log(0.5 ** self.action_dim)\n","        q1_sampling = torch.cat([q1_random - random_density, q1_policy - policy_actions_log_prob.detach()], 1)\n","        q2_sampling = torch.cat([q2_random - random_density, q2_policy - policy_actions_log_prob.detach()], 1)\n","\n","        cql_q1_loss = torch.logsumexp(q1_sampling, dim=1).mean() - q_1_pred.mean()\n","        cql_q2_loss = torch.logsumexp(q2_sampling, dim=1).mean() - q_2_pred.mean()\n","\n","\n","        # alpha_cql optimization\n","        alpha_cql = torch.clamp(self.log_alpha_cql.exp(), min=0.0, max=1000000.0)\n","        cql_q1_loss_alpha = alpha_cql * (cql_q1_loss - self.cql_threshold)\n","        cql_q2_loss_alpha = alpha_cql * (cql_q2_loss - self.cql_threshold)\n","        \n","        self.alpha_cql_optimizer.zero_grad()\n","        alpha_cql_loss = (- cql_q1_loss_alpha - cql_q2_loss_alpha) * 0.5\n","        alpha_cql_loss.backward(retain_graph=True)\n","        self.alpha_cql_optimizer.step()\n","\n","        # update losses\n","        qf_1_loss = qf_1_loss_mse + cql_q1_loss_alpha\n","        qf_2_loss = qf_2_loss_mse + cql_q2_loss_alpha\n","        # ---------------------------------------------------------------------------------------------------------------------------------------------------- #\n","            \n","        # train Q functions and actor\n","        self.qf_1_optimizer.zero_grad()\n","        self.qf_2_optimizer.zero_grad()\n","        self.actor_optimizer.zero_grad()\n","\n","        qf_1_loss.backward(retain_graph=True)\n","        qf_2_loss.backward(retain_graph=True)\n","        actor_loss.backward(retain_graph=False)\n","\n","        self.qf_1_optimizer.step()\n","        self.qf_2_optimizer.step()\n","        self.actor_optimizer.step()\n","\n","        # target update (qf1 and qf2)\n","        if self.total_step % self.policy_update_freq == 0:\n","            self._target_soft_update()\n","        \n","        return actor_loss.detach().cpu().numpy(), qf_1_loss.detach().cpu().numpy(), qf_2_loss.detach().cpu().numpy(), alpha_loss.detach().cpu().numpy(), alpha_cql_loss.detach().cpu().numpy()\n","    \n","    def train(self, num_frames: int, save_every: int, test_env, writer, load_from_checkpoint: bool = False):\n","        \"\"\"Train the agent.\"\"\"\n","\n","        start_frame = 1\n","\n","        if load_from_checkpoint:\n","            cp = torch.load(CHECKPOINT_PATH + \".tar\")\n","\n","            start_frame = cp[\"frame\"]\n","\n","            self.actor.load_state_dict(cp[\"actor_network\"])\n","            self.qf_1.load_state_dict(cp[\"qf1_network\"])\n","            self.qf_2.load_state_dict(cp[\"qf2_network\"])\n","            self.qf_1_target.load_state_dict(cp[\"qf1_target_network\"])\n","            self.qf_2_target.load_state_dict(cp[\"qf2_target_network\"])\n","\n","            self.log_alpha = cp[\"log_alpha\"]\n","            self.log_alpha_cql = cp[\"log_alpha_cql\"]\n","\n","            self.actor_optimizer.load_state_dict(cp[\"actor_optimizer\"])\n","            self.qf_1_optimizer.load_state_dict(cp[\"qf1_optimizer\"])\n","            self.qf_2_optimizer.load_state_dict(cp[\"qf2_optimizer\"])\n","            self.alpha_optimizer.load_state_dict(cp[\"alpha_optimizer\"])\n","            self.alpha_cql_optimizer.load_state_dict(cp[\"alpha_cql_optimizer\"])\n","        \n","        for self.total_step in range(start_frame, num_frames + 1):\n","            print(\"\\rframe: \" + str(self.total_step), end=\"\")\n","            actor_loss, qf1_loss, qf2_loss, alpha_loss, alpha_cql_loss = self.update_model()\n","\n","            writer.add_scalar(\"Actor_loss\", actor_loss, self.total_step)\n","            writer.add_scalar(\"Qf1_loss\", qf1_loss, self.total_step)\n","            writer.add_scalar(\"Qf2_loss\", qf2_loss, self.total_step)\n","            writer.add_scalar(\"Alpha_loss\", alpha_loss, self.total_step)\n","            writer.add_scalar(\"Alpha_cql_loss\", alpha_cql_loss, self.total_step)\n","\n","            # save the model every SAVE_EVERY steps\n","            if self.total_step % save_every == 0:\n","                self._save_checkpoint()\n","                self.test(test_env, writer)\n","            \n","        \n","        self._save_checkpoint()\n","    \n","    def _save_checkpoint(self):\n","        torch.save({\n","            \"frame\":                self.total_step,\n","            \"actor_network\":        self.actor.state_dict(),\n","            \"qf1_network\":          self.qf_1.state_dict(),\n","            \"qf2_network\":          self.qf_2.state_dict(),\n","            \"qf1_target_network\":   self.qf_1_target.state_dict(),\n","            \"qf2_target_network\":   self.qf_2_target.state_dict(),\n","            \"log_alpha\":            self.log_alpha,\n","            \"log_alpha_cql\":        self.log_alpha_cql,\n","            \"actor_optimizer\":      self.actor_optimizer.state_dict(),\n","            \"qf1_optimizer\":        self.qf_1_optimizer.state_dict(),\n","            \"qf2_optimizer\":        self.qf_2_optimizer.state_dict(),\n","            \"alpha_optimizer\":      self.alpha_optimizer.state_dict(),\n","            \"alpha_cql_optimizer\":  self.alpha_cql_optimizer.state_dict(),\n","        }, CHECKPOINT_PATH + \".tar\")\n","\n","        torch.save({\n","            \"frame\":                self.total_step,\n","            \"actor_network\":        self.actor.state_dict(),\n","            \"qf1_network\":          self.qf_1.state_dict(),\n","            \"qf2_network\":          self.qf_2.state_dict(),\n","            \"qf1_target_network\":   self.qf_1_target.state_dict(),\n","            \"qf2_target_network\":   self.qf_2_target.state_dict(),\n","            \"log_alpha\":            self.log_alpha,\n","            \"log_alpha_cql\":        self.log_alpha_cql,\n","            \"actor_optimizer\":      self.actor_optimizer.state_dict(),\n","            \"qf1_optimizer\":        self.qf_1_optimizer.state_dict(),\n","            \"qf2_optimizer\":        self.qf_2_optimizer.state_dict(),\n","            \"alpha_optimizer\":      self.alpha_optimizer.state_dict(),\n","            \"alpha_cql_optimizer\":  self.alpha_cql_optimizer.state_dict(),\n","        }, CHECKPOINT_PATH + str(self.total_step) + \".tar\")\n","        \n","    def test(self, test_env, writer):\n","        \"\"\"Test the agent.\"\"\"\n","        \n","        state = test_env.reset()\n","        done = False\n","        score = 0\n","        \n","        while not done:\n","            action = self.select_action(state)\n","            next_state, reward, done, _ = test_env.step(action)\n","\n","            state = next_state\n","            score += reward\n","        \n","        print(\"score: \", score)\n","        writer.add_scalar(\"Eval_score\", score, self.total_step)\n","    \n","    def _target_soft_update(self):\n","        \"\"\"Soft-update: target = tau*local + (1-tau)*target.\"\"\"\n","        tau = self.tau\n","        \n","        for t_param, l_param in zip(self.qf_1_target.parameters(), self.qf_1.parameters()):\n","            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n","        \n","        for t_param, l_param in zip(self.qf_2_target.parameters(), self.qf_2.parameters()):\n","            t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nkNORY_UrM-5"},"source":["## Environment\n"]},{"cell_type":"code","metadata":{"id":"Tdbkq6V-rM-5"},"source":["# environment\n","env = gym.make(\"ant-bullet-medium-v0\")\n","dataset = env.get_dataset()\n","\n","def sample_batch(batch_size: int):\n","    l = len(dataset[\"actions\"])\n","\n","    indeces = np.asarray(random.sample(range(l - 1), batch_size))\n","\n","    for i in range(len(indeces)):\n","        if dataset[\"terminals\"][indeces[i]] == 1:\n","            indeces[i] -= 1\n","\n","    res = {\n","        \"state\": dataset[\"observations\"][indeces],\n","        \"action\": dataset[\"actions\"][indeces],\n","        \"next_state\": dataset[\"observations\"][indeces + 1],\n","        \"reward\": dataset[\"rewards\"][indeces],\n","        \"done\": dataset[\"terminals\"][indeces]\n","    }\n","\n","    return res\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NCg_8vm_rM-5"},"source":["## Initialize"]},{"cell_type":"code","metadata":{"id":"THIVfjdWrM-6"},"source":["# parameters\n","num_frames = int(5e6)\n","save_every = num_frames//100\n","batch_size = 256\n","writer = SummaryWriter(TENSORBOARD_PATH + TEST_NAME)\n","\n","agent = SACAgent(\n","    dataset, \n","    state_dim=env.observation_space.shape[0], \n","    action_dim=env.action_space.shape[0], \n","    batch_size=batch_size, \n","    random_actions_num=10, \n","    cql_threshold=10\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Kq_NwN5rM-6"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"b2d85nMNrM-7"},"source":["agent.train(num_frames, save_every, test_env=env, load_from_checkpoint=False, writer=writer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sAM__WeP5aOn"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"tS5u8BMm5Zvf"},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\r\n","!apt-get install x11-utils > /dev/null 2>&1\r\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOnZjFEj5jqP"},"source":["from gym import logger as gymlogger\r\n","from gym.wrappers import Monitor\r\n","gymlogger.set_level(40) #error only\r\n","import glob\r\n","import io\r\n","import os\r\n","import base64\r\n","from IPython.display import HTML\r\n","from IPython import display as ipythondisplay\r\n","import time\r\n","\r\n","from pyvirtualdisplay import Display\r\n","display = Display(visible=0, size=(1400, 900))\r\n","display.start()\r\n","\r\n","\"\"\"\r\n","Utility functions to enable video recording of gym environment and displaying it\r\n","To enable video, just do \"env = wrap_env(env)\"\"\r\n","\"\"\"\r\n","def show_video():\r\n","    mp4list = glob.glob('videos/*/*.mp4')\r\n","    mp4list.sort(key=os.path.getmtime)\r\n","    if len(mp4list) > 0:\r\n","        mp4 = mp4list[-1]\r\n","        video = io.open(mp4, 'rb').read()\r\n","        encoded = base64.b64encode(video)\r\n","        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \r\n","                    loop controls style=\"height: 400px;\">\r\n","                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\r\n","                </video>'''.format(encoded.decode('ascii'))))\r\n","    else: \r\n","        print(\"Could not find video\")\r\n","    \r\n","\r\n","def wrap_env(env):\r\n","    env = Monitor(env, './videos/' + str(time.time()) + '/')  # Monitor objects are used to save interactions as videos\r\n","    return env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BNnXaVhV5kkv"},"source":["for e in range(5):\r\n","  test_env = wrap_env(gym.make(\"ant-bullet-medium-v0\"))\r\n","  state = test_env.reset()\r\n","  done = False\r\n","  score = 0\r\n","\r\n","  while not done:\r\n","      test_env.render()\r\n","\r\n","      action = agent.select_action(state)\r\n","      next_state, reward, done, _ = test_env.step(action)\r\n","\r\n","      state = next_state\r\n","      score += reward\r\n","\r\n","  test_env.close()\r\n","  show_video()\r\n","\r\n","  print(\"score: \", score)"],"execution_count":null,"outputs":[]}]}