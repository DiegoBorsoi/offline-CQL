{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Azure - Offline CQL - QR-DQN - mspacman.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU","kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Wb6QUwP36BfO"},"source":["## Initial Settings"]},{"cell_type":"code","metadata":{"id":"YpG69IRBpmtX"},"source":["!pip install gym > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-mxmtQwu16EA","gather":{"logged":1613152438501}},"source":["# for the datasets\r\n","!pip install git+https://github.com/takuseno/d4rl-atari > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwMtKFProBmv","gather":{"logged":1613152466342}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import torch.optim as optim\n","from torch.nn.utils import clip_grad_norm_\n","import random\n","import math\n","from torch.utils.tensorboard import SummaryWriter\n","from collections import deque, namedtuple\n","import time\n","import gym\n","import torchvision.transforms as trans"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJy7o-bQGl2d","gather":{"logged":1613152476273}},"source":["#from google.colab import drive\r\n","#drive.mount(\"./drive\")\r\n","\r\n","import os\r\n","\r\n","BASE_PATH = \"./Saves/\"\r\n","\r\n","try:\r\n","  os.mkdir(BASE_PATH)\r\n","except:\r\n","  pass\r\n","\r\n","TEST_NAME = \"CQL-pacman\"\r\n","\r\n","try:\r\n","  os.mkdir(BASE_PATH + \"checkpoints/\")\r\n","except:\r\n","  pass\r\n","\r\n","CHECKPOINT_PATH = BASE_PATH + \"checkpoints/\" + TEST_NAME\r\n","TENSORBOARD_PATH = BASE_PATH + \"runs/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bOYMRSlB6Ddr"},"source":["## Environment"]},{"cell_type":"code","metadata":{"id":"6SnED0En2IBX","gather":{"logged":1613152502820}},"source":["import d4rl_atari\r\n","\r\n","env = gym.make('ms-pacman-expert-v0')\r\n","video_env = gym.make(\"MsPacman-v0\")  # environment used for test and visualization\r\n","\r\n","# clear dataset variable\r\n","dataset = 0\r\n","# load dataset\r\n","dataset = env.get_dataset()\r\n","\r\n","\r\n","def sampleDataset(batch_size, device):\r\n","  \"\"\"Randomly sample a batch of experiences from the dataset.\"\"\"\r\n","  indeces = random.sample(range(len(dataset['observations']) - 1), k=batch_size)\r\n","\r\n","  # to avoid final states\r\n","  for i in range(batch_size):\r\n","    for j in range(4):\r\n","      if dataset['terminals'][indeces[i-j]] == 1:\r\n","        indeces[i] -= j+1                         # WARNING: every episode needs to be at least 4 frames\r\n","\r\n","  states = torch.from_numpy(np.stack([np.vstack([dataset['observations'][i], dataset['observations'][i-1], dataset['observations'][i-2], dataset['observations'][i-3]]) for i in indeces])).float().to(device)\r\n","  next_states = torch.from_numpy(np.stack([np.vstack([dataset['observations'][i+1], dataset['observations'][i], dataset['observations'][i-1], dataset['observations'][i-2]]) for i in indeces])).float().to(device)\r\n","\r\n","  actions = torch.from_numpy(np.vstack([dataset['actions'][i] for i in indeces])).long().to(device)\r\n","  rewards = torch.from_numpy(np.vstack([dataset['rewards'][i] for i in indeces])).float().to(device)\r\n","  dones = torch.from_numpy(np.vstack([dataset['terminals'][i] for i in indeces]).astype(np.uint8)).float().to(device)\r\n","\r\n","  return (states, actions, rewards, next_states, dones)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r8EIBx-L6H_3"},"source":["## Network"]},{"cell_type":"code","metadata":{"id":"rxGN_KnHoBm5","gather":{"logged":1613152505094}},"source":["class QR_DQN(nn.Module):\n","    def __init__(self, state_size, action_size, seed, N):\n","        super(QR_DQN, self).__init__()\n","        self.seed = torch.manual_seed(seed)\n","        self.input_shape = state_size\n","        self.action_size = action_size\n","        self.N = N\n","\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n","            nn.ReLU()\n","        )\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU()\n","        )\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU()\n","        )\n","        self.ff_1 = nn.Linear(64*7*7, 512)\n","        self.ff_2 = nn.Linear(512, action_size * N)\n","    \n","    def forward(self, x):\n","\n","        # x.shape = (BATCH_SIZE, 4, 84, 84)\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        \n","        x = x.view(-1, 64*7*7)\n","        x = torch.relu(self.ff_1(x))\n","        out = torch.relu(self.ff_2(x))\n","        \n","        return out.view(x.shape[0], self.N, self.action_size)\n","    \n","    def get_action(self,input):\n","        x = self.forward(input)\n","        return x.mean(dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oG4qfLTF6L2Q"},"source":["## QR-DQN Agent"]},{"cell_type":"code","metadata":{"id":"Z5Q0ZWDwoBm7","gather":{"logged":1613152508739}},"source":["class QRDQN_Agent():\n","    \"\"\"Interacts with and learns from the environment.\"\"\"\n","\n","    def __init__(self,\n","                 state_size,\n","                 action_size,\n","                 BATCH_SIZE,\n","                 N,\n","                 LR,\n","                 EPS_ADAM,\n","                 TAU,\n","                 GAMMA,\n","                 ALPHA,\n","                 UPDATE_TARGET_NETWORK_STEPS,\n","                 device,\n","                 seed):\n","        \"\"\"Initialize an Agent object.\n","        \n","        Params\n","        ======\n","            state_size (int): dimension of each state\n","            action_size (int): dimension of each action\n","            BATCH_SIZE (int): size of the training batch\n","            N (int): number of heads of the training network\n","            LR (float): learning rate\n","            EPS_ADAM (float): epsilon used by the optimizer ADAM\n","            TAU (float): tau for soft updating the network weights\n","            GAMMA (float): discount factor\n","            ALPHA (float): weight of the log_sum_exp part of the loss\n","            UPDATE_TARGET_NETWORK_STEPS (int): steps between each update of the target network\n","            device (str): device that is used for the compute\n","            seed (int): random seed\n","        \"\"\"\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.BATCH_SIZE = BATCH_SIZE\n","        self.N = N\n","        self.TAU = TAU\n","        self.GAMMA = GAMMA\n","        self.ALPHA = ALPHA\n","        self.update_target_network_count = 0  # count of steps to use for target network update\n","        self.UPDATE_TARGET_NETWORK_STEPS = UPDATE_TARGET_NETWORK_STEPS\n","        self.seed = random.seed(seed)\n","        self.device = device\n","\n","        # create the tensor containing the quantiles values\n","        self.quantile_tau = torch.FloatTensor([i/self.N for i in range(1,self.N+1)]).to(device)\n","\n","        # initialize the action retainer variables\n","        self.action_step = 4\n","        self.last_action = None\n","\n","        # Q-Networks\n","        self.qnetwork_local = QR_DQN(state_size, action_size, seed, self.N).to(device)  # main network\n","        self.qnetwork_target = QR_DQN(state_size, action_size, seed, self.N).to(device) # auxiliary network\n","\n","        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR, eps=EPS_ADAM)\n","        print(self.qnetwork_local)\n","    \n","    def step(self, writer, frame):\n","        # sample the dataset for BATCH_SIZE transactions\n","        experiences = sampleDataset(self.BATCH_SIZE, self.device)\n","        # learn from this batch and calculate the loss\n","        loss = self.learn(experiences)\n","        writer.add_scalar(\"Q_loss\", loss, frame)\n","\n","    def act(self, state, eps=0.):\n","        \"\"\"Returns actions for given state as per current policy. Acting only every 4 frames!\n","        \n","        Params\n","        ======\n","            frame: to adjust epsilon\n","            state (array_like): current state\n","            \n","        \"\"\"\n","\n","        if self.action_step == 4:\n","\n","            state = state.float().unsqueeze(0).to(self.device)\n","            self.qnetwork_local.eval()\n","            with torch.no_grad():\n","                action_values = self.qnetwork_local.get_action(state)\n","            self.qnetwork_local.train()\n","\n","            # Epsilon-greedy action selection\n","            if random.random() > eps: # select greedy action if random number is higher than epsilon or noisy network is used!\n","                action = np.argmax(action_values.cpu().data.numpy())\n","                self.last_action = action\n","                return action\n","            else:\n","                action = random.choice(np.arange(self.action_size))\n","                self.last_action = action \n","                return action\n","            self.action_step = 0\n","        else:\n","            self.action_step += 1\n","            return self.last_action\n","\n","    def learn(self, experiences):\n","        \"\"\"Update value parameters using given batch of experience tuples.\n","        Params\n","        ======\n","            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n","        \"\"\"\n","        self.optimizer.zero_grad()\n","        states, actions, rewards, next_states, dones = experiences\n","\n","        # Get max predicted Q values (for next states) from target model\n","        Q_targets_next = self.qnetwork_target(next_states).detach().cpu() #(BATCH_SIZE, N, action_size)\n","        action_indx = torch.argmax(Q_targets_next.mean(dim=1), dim=1, keepdim=True) # predicted action for each state # (BATCH_SIZE, 1)\n","\n","        action_indx = action_indx.unsqueeze(-1).expand(self.BATCH_SIZE, self.N, 1)  # (BATCH_SIZE, N, 1)\n","        Q_targets_next = Q_targets_next.gather(2, action_indx).transpose(1,2)   # (BATCH_SIZE, 1, N)  \n","\n","        assert Q_targets_next.shape == (self.BATCH_SIZE, 1, self.N)\n","\n","        # Compute Q targets for current states \n","        Q_targets = rewards.unsqueeze(-1) + (self.GAMMA * Q_targets_next.to(self.device) * (1 - dones.unsqueeze(-1)))  # (BATCH_SIZE, 1, N)\n","        # Get expected Q values from local model\n","        Q_expected_actions = self.qnetwork_local(states)  # (BATCH_SIZE, N, action_size)\n","        Q_expected = Q_expected_actions.gather(2, actions.unsqueeze(-1).expand(self.BATCH_SIZE, self.N, 1))  # (BATCH_SIZE, N, 1)\n","        \n","        # Compute loss\n","        td_error = Q_targets - Q_expected\n","        assert td_error.shape == (self.BATCH_SIZE, self.N, self.N), \"wrong td error shape\"\n","        huber_l = calculate_huber_loss(td_error, 1.0)\n","        quantil_l = abs(self.quantile_tau - (td_error.detach() < 0).float()) * huber_l / 1.0\n","\n","        loss = quantil_l.mean(dim=2).sum(dim=1) # (BATCH_SIZE)\n","        loss = loss.mean()  # mean between batch values\n","        \n","        # ---- CQL extension --------------------------------------------------------------------------------------------------------------------\n","        Q_expected_actions_single = Q_expected_actions.mean(dim=1)  # (BATCH_SIZE, action_size)\n","\n","        log_sum_exp = torch.logsumexp(Q_expected_actions_single, dim=1)  # (BATCH_SIZE)\n","        alpha_term = log_sum_exp - Q_expected_actions_single.gather(1, actions).squeeze() # (BATCH_SIZE)\n","        alpha_term = alpha_term.mean()  # mean over BATCH_SIZE\n","        # ---------------------------------------------------------------------------------------------------------------------------------------\n","\n","        # CQL(H)\n","        loss = self.ALPHA * alpha_term + loss # * 0.5\n","        \n","        # Minimize the loss\n","        loss.backward()\n","        \n","        self.optimizer.step()\n","\n","        # ------------------- update target network ------------------- #\n","        self.update_target_network_count += 1\n","        if self.update_target_network_count == self.UPDATE_TARGET_NETWORK_STEPS:\n","          self.soft_update(self.qnetwork_local, self.qnetwork_target)\n","        # ------------------------------------------------------------- #\n","\n","        return loss.detach().cpu().numpy()            \n","\n","    def soft_update(self, local_model, target_model):\n","        \"\"\"Soft update model parameters.\n","        θ_target = τ*θ_local + (1 - τ)*θ_target\n","        Params\n","        ======\n","            local_model (PyTorch model): weights will be copied from\n","            target_model (PyTorch model): weights will be copied to\n","        \"\"\"\n","        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n","            target_param.data.copy_(self.TAU*local_param.data + (1.0-self.TAU)*target_param.data)\n","            \n","def calculate_huber_loss(td_errors, k=1.0):\n","    \"\"\"\n","    Calculate huber loss element-wisely depending on k.\n","    \"\"\"\n","    loss = torch.where(td_errors.abs() <= k, 0.5 * td_errors.pow(2), k * (td_errors.abs() - 0.5 * k))\n","    return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ic8S9A616YN7"},"source":["## Run declaration"]},{"cell_type":"code","metadata":{"id":"0dkImGw7SEJi","gather":{"logged":1613152513449}},"source":["def run(frames=10000, save_every=10000, load_from_checkpoint=False):\r\n","    \"\"\"Quantile Regression - Deep Q-Learning.\r\n","    \r\n","    Params\r\n","    ======\r\n","        frames (int): maximum number of training frames\r\n","        save_every (int): number of frame between each save\r\n","    \"\"\"\r\n","    \r\n","    start_frame = 1\r\n","\r\n","    # if we want to continue the training, load the old checkpoint\r\n","    if load_from_checkpoint:\r\n","      checkpoint = torch.load(CHECKPOINT_PATH + \".tar\")\r\n","\r\n","      start_frame = checkpoint[\"frame\"]\r\n","      agent.qnetwork_local.load_state_dict(checkpoint[\"local_network\"])\r\n","      agent.qnetwork_target.load_state_dict(checkpoint[\"target_network\"])\r\n","      agent.optimizer.load_state_dict(checkpoint[\"optim\"])\r\n","\r\n","    for frame in range(start_frame, frames+1):\r\n","      agent.step(writer, frame)\r\n","\r\n","      print(\"\\rFrame {} \".format(frame), end=\"\")\r\n","\r\n","      # every save_every frames we save the state_dict in a file\r\n","      if frame % save_every == 0:\r\n","        # save the checkpoint\r\n","        torch.save({\r\n","            \"frame\": frame,\r\n","            \"local_network\": agent.qnetwork_local.state_dict(),\r\n","            \"target_network\": agent.qnetwork_target.state_dict(),\r\n","            \"optim\": agent.optimizer.state_dict()\r\n","        }, CHECKPOINT_PATH + str(frame) + \".tar\")\r\n","        torch.save({\r\n","            \"frame\": frame,\r\n","            \"local_network\": agent.qnetwork_local.state_dict(),\r\n","            \"target_network\": agent.qnetwork_target.state_dict(),\r\n","            \"optim\": agent.optimizer.state_dict()\r\n","        }, CHECKPOINT_PATH + \".tar\")\r\n","\r\n","        # every 5000 frames evaluate the agent\r\n","        agent.qnetwork_local.eval()\r\n","\r\n","        state = env.reset()\r\n","        done = False\r\n","        score = 0\r\n","        count = 0\r\n","\r\n","        prev_state = state.copy()\r\n","        last_max_states = deque(maxlen=4) # the first is the newest\r\n","        for i in range(3):\r\n","          last_max_states.appendleft(state.copy())\r\n","\r\n","        while not done:\r\n","\r\n","          last_max_states.appendleft(np.maximum(state, prev_state))\r\n","\r\n","          states_quadr = torch.stack([torch.from_numpy(x) for x in last_max_states])\r\n","          action = agent.act(states_quadr, 0)\r\n","          next_state, reward, done, _ = env.step(action)\r\n","\r\n","          prev_state = state\r\n","          state = next_state\r\n","          score += reward\r\n","\r\n","          count += 1\r\n","          if count == 1000 and score == 0:\r\n","            break\r\n","\r\n","        writer.add_scalar(\"Eval Score\", score, frame)\r\n","        print(\"\\rFrame {} \\tScore: {}\".format(frame, score))\r\n","\r\n","        agent.qnetwork_local.train()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTbugsX552Pe"},"source":["## Initialization and random seed settings"]},{"cell_type":"code","metadata":{"id":"BUG-U-FCoBnB","gather":{"logged":1613152587729}},"source":["writer = SummaryWriter(TENSORBOARD_PATH + TEST_NAME)\n","seed = 1\n","BATCH_SIZE = 32\n","N = 200\n","GAMMA = 0.99\n","TAU = 1   # update the target network coping the local network\n","LR = 5e-5\n","EPS_ADAM = 0.01/32\n","ALPHA = 1.0\n","UPDATE_TARGET_NETWORK_STEPS = 2000\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using \", device)\n","\n","\n","np.random.seed(seed)\n","\n","print('State space: {}'.format(env.observation_space.shape))\n","print('Action space: {}'.format(env.action_space.n))\n","\n","env.seed(seed)\n","action_size = env.action_space.n\n","state_size = env.observation_space.shape\n","\n","agent = QRDQN_Agent(state_size=state_size,    \n","                  action_size=action_size,\n","                  BATCH_SIZE=BATCH_SIZE,\n","                  N=N, \n","                  LR=LR, \n","                  EPS_ADAM=EPS_ADAM, \n","                  TAU=TAU, \n","                  GAMMA=GAMMA,\n","                  ALPHA=ALPHA,\n","                  UPDATE_TARGET_NETWORK_STEPS=UPDATE_TARGET_NETWORK_STEPS,\n","                  device=device, \n","                  seed=seed)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BsMV7Xrg6fgs"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"nsX7JB0JSbCC","gather":{"logged":1612883009871}},"source":["TOTAL_FRAMES = int(10e6)\r\n","SAVE_EVERY = TOTAL_FRAMES//100\r\n","\r\n","t0 = time.time()\r\n","run(frames= TOTAL_FRAMES, save_every= SAVE_EVERY, load_from_checkpoint=False)\r\n","t1 = time.time()\r\n","\r\n","print(\"\\nTraining time: {}min\".format(round((t1-t0)/60,2)))\r\n","\r\n","# save the checkpoint\r\n","torch.save({\r\n","    \"frame\": TOTAL_FRAMES,\r\n","    \"local_network\": agent.qnetwork_local.state_dict(),\r\n","    \"target_network\": agent.qnetwork_target.state_dict(),\r\n","    \"optim\": agent.optimizer.state_dict()\r\n","}, CHECKPOINT_PATH + \".tar\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aImQEE2c6g2o"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"AJrCcpZA6jTQ"},"source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\r\n","!apt-get install x11-utils > /dev/null 2>&1\r\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxlRcQSd6mRa"},"source":["from gym import logger as gymlogger\r\n","from gym.wrappers import Monitor\r\n","gymlogger.set_level(40) #error only\r\n","import glob\r\n","import io\r\n","import os\r\n","import base64\r\n","from IPython.display import HTML\r\n","from IPython import display as ipythondisplay\r\n","import time\r\n","\r\n","from pyvirtualdisplay import Display\r\n","display = Display(visible=0, size=(1400, 900))\r\n","display.start()\r\n","\r\n","\"\"\"\r\n","Utility functions to enable video recording of gym environment and displaying it\r\n","To enable video, just do \"env = wrap_env(env)\"\"\r\n","\"\"\"\r\n","def show_video():\r\n","    mp4list = glob.glob('videos/*/*.mp4')\r\n","    mp4list.sort(key=os.path.getmtime)\r\n","    if len(mp4list) > 0:\r\n","        mp4 = mp4list[-1]\r\n","        video = io.open(mp4, 'rb').read()\r\n","        encoded = base64.b64encode(video)\r\n","        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \r\n","                    loop controls style=\"height: 400px;\">\r\n","                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\r\n","                </video>'''.format(encoded.decode('ascii'))))\r\n","    else: \r\n","        print(\"Could not find video\")\r\n","    \r\n","\r\n","def wrap_env(env):\r\n","    env = Monitor(env, './videos/' + str(time.time()) + '/')  # Monitor objects are used to save interactions as videos\r\n","    return env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZBOzbo06n1X"},"source":["def preprocessing(imageRGB):\r\n","  transformation = trans.Resize((84, 84))\r\n","\r\n","  imageRGB_torch = torch.Tensor(imageRGB)\r\n","  L = 0.2126 * imageRGB_torch[:, :, 0] + 0.7152 * imageRGB_torch[:, :, 1] + 0.0722 * imageRGB_torch[:, :, 2]\r\n","  trans_image = transformation(L.unsqueeze(0)).squeeze(0)\r\n","  \r\n","  return trans_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Kg1ION17JGB"},"source":["agent.qnetwork_local.eval()\r\n","\r\n","for e in range(5):\r\n","  video_env = wrap_env(video_env)\r\n","  state = video_env.reset()\r\n","  done = False\r\n","  score = 0\r\n","\r\n","  prev_state = state.copy()\r\n","  last_max_states = deque(maxlen=4) # the first is the newest\r\n","  for i in range(3):\r\n","    last_max_states.appendleft(state.copy())\r\n","\r\n","  count = 0\r\n","  while not done:\r\n","    video_env.render()\r\n","\r\n","    last_max_states.appendleft(np.maximum(state, prev_state))\r\n","\r\n","    states_quadr = torch.stack([preprocessing(x) for x in last_max_states])\r\n","    action = agent.act(states_quadr, 0)\r\n","\r\n","    if count == 0:\r\n","      action = 1\r\n","      count += 1\r\n","\r\n","    next_state, reward, done, _ = video_env.step(action)\r\n","\r\n","    prev_state = state\r\n","    state = next_state\r\n","    score += reward\r\n","\r\n","  video_env.close()\r\n","  show_video()\r\n","\r\n","  print('Test episode {} - R(tau) = {}'.format(e, score))"],"execution_count":null,"outputs":[]}]}